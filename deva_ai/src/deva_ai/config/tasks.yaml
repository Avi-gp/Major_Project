IngestionTask:
  description: >
    Process the dataset located at file path {file_path} with filename {file_name}.
    Read the file, analyze its content, and provide insights about the dataset.
  expected_output: >
    A dictionary containing the dataset information. Return the results in a valid JSON format with the following required keys:
    - "dataset_shape": [rows, columns] - The shape of the dataset as a list of two integers
    - "preview": [...] - A list containing dictionaries representing the first few rows
    - "dtypes": {...} - A dictionary mapping column names to their data types

PreprocessingTask:
  description: >
    Clean and preprocess the dataset located at file path {file_path} with filename {file_name}.
    Apply comprehensive data cleaning techniques including:
    1. Data Type Conversion for numeric columns stored as objects
    2. Handle Missing Values using appropriate imputation strategies
    3. Detect and Remove Duplicate Records
    4. Identify and Drop Irrelevant Columns
    5. Clean Special Characters in Categorical Columns
    6. Advanced Type Inference for Datetime Columns
    7. Outlier Detection and Treatment using IQR method
    8. Handle Outliers by Clipping to appropriate bounds
  expected_output: >
    A dictionary containing preprocessing statistics and information. Return the results in a valid JSON format with the following required keys:
    - "original_shape": [rows, columns] - Original dataset shape
    - "final_shape": [rows, columns] - Final dataset shape after preprocessing
    - "original_missing_values": int - Count of missing values before preprocessing
    - "missing_values_handled": int - Count of missing values that were handled
    - "duplicates_removed": int - Count of duplicate rows removed
    - "outliers_handled": int - Count of outliers that were handled
    - "columns_dropped": [{"column": "column_name", "reason": "reason for dropping"}] - List of dictionaries containing column names and reasons they were dropped
    - "transformations_applied": [...] - List of transformations that were applied
    - "column_type_changes": {...} - Dictionary with column names as keys and nested dictionaries containing "original" and "new" types
    - "columns_dtypes": {...} - Dictionary mapping column names to their final data types
    - "dataset_preview": [...] - List of dictionaries representing the first few rows of the preprocessed dataset
    - "preprocessed_file_path": string - Path to the saved preprocessed file
    - "preprocessed_file_name": string - Name of the saved preprocessed file
    Save the preprocessed dataset to a new file with the preprocessing steps applied.

FeatureEngineeringTask:
  description: >
    Engineer features from the preprocessed dataset located at file path {preprocessed_file_path} with filename {preprocessed_file_name}.
    Apply comprehensive feature engineering techniques including:
    1. Numerical Feature Scaling:
       - Standardization (z-score normalization)
       - Min-Max scaling (normalization to [0,1] range)
       - Robust scaling for data with outliers
    2. Categorical Feature Encoding:
       - One-hot encoding for nominal categories
       - Label encoding for ordinal categories
       - Target encoding for high-cardinality categories
    3. Feature Selection:
       - Correlation-based feature selection
       - Variance threshold filtering
       - Feature importance evaluation using Random Forest
       - Suggest features that could be dropped (rather than automatically dropping them)
  expected_output: >
    A dictionary containing feature engineering statistics and information. Return the results in a valid JSON format with the following required keys:
    - "original_shape": [rows, columns] - Shape before feature engineering
    - "final_shape": [rows, columns] - Shape after feature engineering
    - "numerical_features": [...] - List of numerical feature names
    - "categorical_features": [...] - List of categorical feature names
    - "scaling_methods_applied": {...} - Dictionary mapping numerical columns to scaling methods applied
    - "encoding_methods_applied": {...} - Dictionary mapping categorical columns to encoding methods applied
    - "target_column": string - Name of the target column used for feature importance evaluation
    - "feature_importance": {...} - Dictionary mapping feature names to their importance scores
    - "suggested_features_to_drop": [{"feature": "feature_name", "reason": "reason for suggesting to drop"}] - List of dictionaries with feature names and reasons they could be dropped
    - "high_correlation_pairs": [[feature1, feature2, correlation_value], ...] - List of highly correlated feature pairs
    - "engineered_preview": [...] - List of dictionaries representing the first few rows of the engineered dataset
    - "engineered_file_path": string - Path to the saved engineered features file
    Save the transformed dataset to a new file with all engineering steps applied.