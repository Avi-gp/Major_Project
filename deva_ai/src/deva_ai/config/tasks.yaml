IngestionTask:
  description: >
    Process the dataset located at file path {file_path} with filename {file_name}.
    Read the file, analyze its content, and provide insights about the dataset.
  expected_output: >
    A dictionary containing the dataset information. Return the results in a valid JSON format with the following required keys:
    - "dataset_shape": [rows, columns] - The shape of the dataset as a list of two integers
    - "preview": [...] - A list containing dictionaries representing the first few rows
    - "dtypes": {...} - A dictionary mapping column names to their data types

PreprocessingTask:
  description: >
    Clean and preprocess the dataset located at file path {file_path} with filename {file_name}.
    Apply comprehensive data cleaning techniques including:
    1. Data Type Conversion for numeric columns stored as objects
    2. Handle Missing Values using appropriate imputation strategies
    3. Detect and Remove Duplicate Records
    4. Identify and Drop Irrelevant Columns
    5. Clean Special Characters in Categorical Columns
    6. Advanced Type Inference for Datetime Columns
    7. Outlier Detection and Treatment using IQR method
    8. Handle Outliers by Clipping to appropriate bounds
  expected_output: >
    A dictionary containing preprocessing statistics and information. Return the results in a valid JSON format with the following required keys:
    - "original_shape": [rows, columns] - Original dataset shape
    - "final_shape": [rows, columns] - Final dataset shape after preprocessing
    - "original_missing_values": int - Count of missing values before preprocessing
    - "missing_values_handled": int - Count of missing values that were handled
    - "duplicates_removed": int - Count of duplicate rows removed
    - "outliers_handled": int - Count of outliers that were handled
    - "columns_dropped": [{"column": "column_name", "reason": "reason for dropping"}] - List of dictionaries containing column names and reasons they were dropped
    - "transformations_applied": [...] - List of transformations that were applied
    - "column_type_changes": {...} - Dictionary with column names as keys and nested dictionaries containing "original" and "new" types
    - "columns_dtypes": {...} - Dictionary mapping column names to their final data types
    - "dataset_preview": [...] - List of dictionaries representing the first few rows of the preprocessed dataset
    - "preprocessed_file_path": string - Path to the saved preprocessed file
    - "preprocessed_file_name": string - Name of the saved preprocessed file
    Save the preprocessed dataset to a new file with the preprocessing steps applied.

FeatureEngineeringTask:
  description: >
    Engineer features from the preprocessed dataset located at file path {preprocessed_file_path} with filename {preprocessed_file_name}.
    Apply comprehensive feature engineering techniques including:
    1. Numerical Feature Scaling:
       - Standardization (z-score normalization)
       - Min-Max scaling (normalization to [0,1] range)
       - Robust scaling for data with outliers
    2. Categorical Feature Encoding:
       - One-hot encoding for binary categories
       - Label encoding for non-binary categories
  expected_output: >
    A dictionary containing feature engineering statistics and information. Return the results in a valid JSON format with the following required keys:
    - "original_shape": [rows, columns] - Shape before feature engineering
    - "final_shape": [rows, engineered_columns] - Shape after feature engineering, where engineered_columns is the total count of numerical_features + categorical_features
    - "numerical_features": [...] - List of numerical feature names
    - "categorical_features": [...] - List of categorical feature names
    - "scaling_methods_applied": {...} - Dictionary mapping numerical columns to scaling methods applied
    - "encoding_methods_applied": {...} - Dictionary mapping categorical columns to encoding methods applied
    - "engineered_preview": [...] - List of dictionaries representing the first few rows of the engineered dataset
    - "engineered_file_path": string - Path to the saved engineered features file
    - "engineered_file_name": string - Name of the saved engineered features file
    Save the transformed dataset to a new file with all engineering steps applied.

InsightGenerationTask:
  description: >
    Generate focused insights from the engineered dataset located at file path {engineered_file_path} 
    with filename {engineered_file_name} and target column {target_column}.
    
    PHASE 1 - DATA ANALYSIS (Tool-dependent):
      Perform the following analyses using tool:
        1. Univariate Analysis:
          - Statistical summaries for numerical features (mean, median, mode, standard deviation, quartiles)
          - Frequency distributions for categorical features
        
        2. Bivariate Analysis:
          - Correlation coefficients between important numerical features
          - Relationships between key feature combinations
        
        3. Categorical Analysis:
          - Category distributions and statistics
          - Numerical distributions across categories (mean, median by group)
        
        4. Correlation Analysis:
          - Correlation matrix with significant relationships
        
        5. Feature Importance Analysis:
          - Random Forest-based feature importance with respect to the {target_column}
        
        6. Target-Based Analysis:
          - Distribution statistics of target variable
          - Target variable relationship with key predictors


    PHASE 2 - AI INTERPRETATION (Independent of tools):
      After analyzing the data, independently generate:
        7. Actionable Insights Generation:
          - Identify key patterns and trends in the data
          - Highlight significant relationships discovered in analyses
          - Note anomalies, outliers, or unexpected findings
          - Identify business-relevant implications of the findings
       
        8. Recommendations Development:
          - Suggest specific actions based on insights
          - Provide data-driven recommendations for strategy
          - Recommend additional analyses or data collection if needed
          - Propose potential model approaches based on findings   

    IMPORTANT: You MUST complete both phases and include both tool-generated analysis 
    results AND independently generated actionable insights and recommendations in your final output.
    Ensure that the insights and recommendations are relevant to the dataset and its context.


  expected_output: >
    A dictionary containing insight generation statistics and analysis results.
    Return the results in a valid JSON format with the following required keys:
    
    - "dataset_summary": {
        "shape": [rows, columns],
        "numerical_features": [...],
        "categorical_features": [...],
        "target_variable": "target_column_name",
        "target_type": "classification_or_regression",
        "missing_values_percentage": float
      }
      
    - "univariate_analysis": {
        "numerical_features": [
          {
            "feature_name": "feature1",
            "statistics": {
              "mean": float,
              "median": float,
              "std": float,
              "min": float,
              "max": float,
              "q1": float,
              "q3": float
            }
          }
        ],
        "categorical_features": [
          {
            "feature_name": "category1",
            "value_counts": {...},
            "unique_values": int,
            "mode": "most_common_value"
          }
        ]
      }
    
    - "bivariate_analysis": {
        "feature_relationships": [
          {
            "features": ["feature1", "feature2"],
            "correlation": float,
            "relationship_type": "linear/non-linear/none"
          }
        ],
        "target_relationships": {
          "feature1": {
            "correlation": float
          }
        }
      }
    
    - "categorical_analysis": {
        "category_distributions": {
          "category1": {
            "most_common": "value",
            "least_common": "value",
            "distribution": {...}
          }
        },
        "numerical_by_category": [
          {
            "numerical": "feature1",
            "categorical": "category1",
            "group_statistics": {
              "category_value1": {"mean": float, "median": float},
              "category_value2": {"mean": float, "median": float}
            }
          }
        ]
      }
    
    - "correlation_analysis": {
        "high_correlation_pairs": [
          {
            "feature1": "feature_name",
            "feature2": "feature_name",
            "correlation": float
          }
        ]
      }
    
    - "feature_importance": {
        "importance_scores": {
          "feature_name": float
        },
        "top_features": ["feature1", "feature2", "feature3"]
      }
    
    - "target_analysis": {
        "target_distribution_statistics": {
          "mean": float,
          "median": float,
          "std": float,
          "min": float,
          "max": float,
          "q1": float,
          "q3": float
        },
        "key_predictors": ["feature1", "feature2"],
        "key_predictor_relationships": {
          "feature1": {
            "correlation": float
          }
        }
      }
    
    - "actionable_insights": {
    "insight1": "description1",
    "insight2": "description2",
    "insight3": "description3",
    ......

      }
    
    - "recommendations":{
        "recommendation1": "description1",
        "recommendation2": "description2",
        "recommendation3": "description3",
        ......
      }